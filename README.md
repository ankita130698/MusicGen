# MusicGen

This repository contains a trainer for the MusicGen model, built and maintained by a single contributor.

## Contributor

**Ankita Govardhan Bondre**

## Usage

### Dataset Creation

Create a folder and place your audio and caption files in it. **They must be in `.wav` and `.txt` format respectively.** You can omit `.txt` files for training with empty text by setting the `--no_label` option to `1`.

![Dataset Example](https://i.imgur.com/AlDlqBI.png)

You can use `.wav` files longer than 30 seconds. In that case, the model will be trained on random crops of the original `.wav` file.

In this example, `segment_000.txt` contains the caption "jazz music, jobim" for the wav file `segment_000.wav`.

### Running the Trainer

Run `python3 run.py --dataset <PATH_TO_YOUR_DATASET>`. Make sure to use the full path to the dataset, not a relative path.

### Options

- `dataset_path`: String, path to your dataset with `.wav` and `.txt` pairs.
- `model_id`: String, MusicGen model to use. Can be `small`/`medium`/`large`. Default: `small`
- `lr`: Float, learning rate. Default: `0.00001`/`1e-5`
- `epochs`: Integer, epoch count. Default: `100`
- `use_wandb`: Integer, `1` to enable wandb, `0` to disable it. Default: `0` = Disabled
- `save_step`: Integer, amount of steps to save a checkpoint. Default: None
- `no_label`: Integer, whether to read a dataset without `.txt` files. Default: `0` = Disabled
- `tune_text`: Integer, perform textual inversion instead of full training. Default: `0` = Disabled
- `weight_decay`: Float, the weight decay regularization coefficient. Default: `0.00001`/`1e-5`
- `grad_acc`: Integer, number of steps to smooth gradients over. Default: 2
- `warmup_steps`: Integer, amount of steps to slowly increase learning rate over to let the optimizer compute statistics. Default: 16
- `batch_size`: Integer, batch size the model sees at once. Reduce to lower memory consumption. Default: 4
- `use_cfg`: Integer, whether to train with some labels randomly dropped out. Default: `0` = Disabled

You can set these options like this: `python3 run.py --use_wandb=1`.

### Models

Once training finishes, the model (and checkpoints) will be available under the `models` folder in the same path you ran the trainer on.

![Model Path Example](https://i.imgur.com/Mu19EPb.png)

To load them, simply run the following in your generation script:

```python
model.lm.load_state_dict(torch.load('models/lm_final.pt'))
```

Where `model` is the MusicGen Object and `models/lm_final.pt` is the path to your model (or checkpoint).

## Citations

```
@article{copet2023simple,
      title={Simple and Controllable Music Generation},
      author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre DÃ©fossez},
      year={2023},
      journal={arXiv preprint arXiv:2306.05284},
}
```
